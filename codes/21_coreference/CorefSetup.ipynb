{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Coreference Resolution\n",
    "\n",
    "This notebook demonstrates how to use the `neuralcoref` library, which integrates with the popular NLP library `spaCy`, to perform coreference resolution. **Coreference resolution** is the task of finding all expressions in a text that refer to the same real-world entity. For example, in \"My sister has a dog. She loves him,\" \"My sister\" and \"She\" refer to the same person, while \"a dog\" and \"him\" refer to the same animal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 1: Environment Setup\n",
    "\n",
    "This markdown cell provides the necessary shell commands to install `neuralcoref` and the small English language model for `spaCy`. The `--no-binary` flag is often used to compile the package from source, which can help resolve compatibility issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install neuralcoref (which will upgrade spacy to 2.1) and re-install spacy models.\n",
    "\n",
    "```sh\n",
    "pip install neuralcoref --no-binary neuralcoref\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 2: Importing Libraries\n",
    "\n",
    "Here, we import the necessary Python libraries. We need `spacy` for the core NLP processing and `neuralcoref` for the coreference resolution functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the main NLP library, spaCy\n",
    "import spacy\n",
    "# Import the neuralcoref library for coreference resolution\n",
    "import neuralcoref"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 3: Initializing the NLP Pipeline\n",
    "\n",
    "This is a crucial step. We first load a `spaCy` English language model. Then, we initialize `neuralcoref` and add it to the `spaCy` processing pipeline. A pipeline is a sequence of components (like a tokenizer, a part-of-speech tagger, a named entity recognizer, etc.) that process a text. By adding `neuralcoref` to this pipeline, we ensure that it will run whenever we process a text with our `nlp` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the small English model from spaCy. \n",
    "# This object, which we call 'nlp', will now contain the entire processing pipeline.\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "# NOTE: The commented line below is an alternative way to load a model, \n",
    "# useful if the simple 'en' shortcut doesn't work.\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Initialize the NeuralCoref model, passing it the vocabulary from our loaded spaCy model.\n",
    "coref = neuralcoref.NeuralCoref(nlp.vocab)\n",
    "\n",
    "# Add the coreference resolution component to the end of the spaCy pipeline.\n",
    "# We give it a name, 'neuralcoref', so we can identify it later.\n",
    "nlp.add_pipe(coref, name='neuralcoref')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 4: Processing Text\n",
    "\n",
    "Now that the pipeline is set up, we can process text. We pass a Unicode string to our `nlp` object. This runs the text through all the components in the pipeline (tokenization, tagging, parsing, and our new coreference resolution). The result is a `Doc` object, which contains the processed text and all its annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the example sentence using our nlp pipeline.\n",
    "# The 'doc' object now contains the text and all the linguistic annotations.\n",
    "doc = nlp(u'My sister has a dog. She loves him.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 5: Understanding Coreference Clusters\n",
    "\n",
    "This markdown cell explains how to access the results from `neuralcoref`. The results are stored in a custom attribute of the `Doc` object called `doc._.coref_clusters`. This attribute holds a list of clusters. Each cluster represents a single real-world entity and contains all the \"mentions\" (spans of text) that refer to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coreference clusters can be found in the `_.coref_clusters` attribute of `doc`. `_.coref_clusters` is a list of mention *clusters* -- each *mention* is a span of tokens in the text and a cluster of such mentions are those spans that co-refer to the same unique *entity*.\n",
    "\n",
    "Each mention is a spacy [Span](https://spacy.io/api/span) object and has all of the methods/attributes of that class.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 6: Inspecting the Coreference Chains\n",
    "\n",
    "This code iterates through the coreference clusters found in our processed document. For each cluster (or \"chain\"), it prints the index of the chain and then iterates through each mention in that chain, printing the mention's text and its start and end token positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each coreference cluster found in the document. \n",
    "# 'enumerate' provides an index for each chain.\n",
    "for idx, chain in enumerate(doc._.coref_clusters):\n",
    "    # Print a header for the current coreference chain.\n",
    "    print (\"Coreference chain %s:\" % idx)\n",
    "    # Loop through each 'mention' (e.g., \"My sister\", \"She\") in the current chain.\n",
    "    for mention in chain.mentions:\n",
    "        # Print the text of the mention, its starting token index, and its ending token index.\n",
    "        print(mention.text, mention.start, mention.end)\n",
    "    # Print a newline for better readability between chains.\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 7: Understanding Syntactic Heads\n",
    "\n",
    "This markdown cell explains the `span.root` attribute. For any span of text (like a mention), the root is the single token that is the syntactic head of that span in the sentence's dependency parse tree. This is useful for understanding the grammatical role of the mention in the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The head of a spacy span can be approximated by the `span.root` attribute, which is \"the token with the shortest path to the root of the sentence.\"  The syntactic relation of the entire mention to the rest of the sentence is best captured by this root."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 8: Extracting Syntactic Information\n",
    "\n",
    "Building on the previous example, this code not only prints the mention's text and position but also extracts more detailed syntactic information about the mention's `root` token: its text, its dependency label (`.dep_`), and its syntactic head (`.head`). This shows, for example, that \"sister\" is the subject (`nsubj`) of \"has\" and \"him\" is the direct object (`dobj`) of \"loves\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through each coreference cluster in the document.\n",
    "for chain in doc._.coref_clusters:\n",
    "    # Loop through each mention within the cluster.\n",
    "    for mention in chain.mentions:\n",
    "        # The following comments describe the attributes we are about to print.\n",
    "        # mention.text = entire text space of entity\n",
    "        # mention.start = token start position of entity\n",
    "        # mention.end = token end position of entity\n",
    "        # mention.root = spacy Token object that is the syntactic head of the mention (in a dependency tree)\n",
    "        \n",
    "        # Print the mention's text, start/end positions, its root token, the root's dependency relationship, and the root's syntactic head token.\n",
    "        print(mention.text, mention.start, mention.end, mention.root, mention.root.dep_, mention.root.head)\n",
    "    # Print a newline for better readability.\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 9: Testing the Limits\n",
    "\n",
    "This markdown cell introduces the next section of the notebook, where we will test the model's performance on more challenging sentences, such as those from the Winograd Schema Challenge, which are designed to be difficult for machines to understand due to ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the limits of spacy coreference. How does it fare on:\n",
    "\n",
    "- Winograd schema challenge questions?\n",
    "- long documents?\n",
    "- near-identity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 10: Creating a Helper Function\n",
    "\n",
    "To make testing easier, we define a simple helper function `print_coref_chains`. This function takes a string of text, runs it through our `nlp` pipeline, and prints the coreference chains in the same way we did before. This saves us from rewriting the loops for each new test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a text string as input.\n",
    "def print_coref_chains(text):\n",
    "    # Process the text with our spaCy pipeline.\n",
    "    doc = nlp(text)\n",
    "    # Loop through the resulting coreference clusters.\n",
    "    for chain in doc._.coref_clusters:\n",
    "        # Loop through each mention in the cluster.\n",
    "        for mention in chain.mentions:\n",
    "            # Print the mention's text and its start/end token positions.\n",
    "            print(mention.text, mention.start, mention.end)\n",
    "        # Print a newline to separate the chains.\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 11: Winograd Schema Test 1\n",
    "\n",
    "This is our first test using the helper function. The sentence is, \"The trophy would not fit in the brown suitcase because **it** was too big.\" The pronoun \"it\" is ambiguous: does it refer to the trophy or the suitcase? A human knows \"it\" refers to the trophy. The model correctly identifies \"The trophy\" and \"it\" as being in the same coreference chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call our helper function with a sentence from the Winograd Schema Challenge.\n",
    "# The model needs to resolve what \"it\" refers to.\n",
    "print_coref_chains(\"The trophy would not fit in the brown suitcase because it was too big\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 12: Winograd Schema Test 2\n",
    "\n",
    "Here, the sentence is, \"The town councilors refused to give the demonstrators a permit because **they** feared violence.\" The pronoun \"they\" could refer to the councilors or the demonstrators. In this context, it refers to the councilors. The model correctly links \"The town councilors\" and \"they\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test another ambiguous sentence. Here, \"they\" is the ambiguous pronoun.\n",
    "print_coref_chains(\"The town councilors refused to give the demonstrators a permit because they feared violence.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell 13: Winograd Schema Test 3\n",
    "\n",
    "This is a variation of the previous sentence: \"The town councilors refused to give the demonstrators a permit because **they** advocated violence.\" By changing one word (\"feared\" to \"advocated\"), the meaning of \"they\" flips to refer to the demonstrators. However, the model **incorrectly** still links \"they\" to \"The town councilors,\" showing the limits of its understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test a variation where the meaning of \"they\" should change.\n",
    "# The model makes an error here, demonstrating a common failure point for these systems.\n",
    "print_coref_chains(\"The town councilors refused to give the demonstrators a permit because they advocated violence.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}