{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Commented and Explained IPython Notebook**\n",
    "\n",
    "This notebook explores linear regression with L2 (Ridge) and L1 (Lasso) regularization, using the movie box office prediction data. Be sure to install beautifulsoup (a great python library for reading XML).\n",
    "\n",
    "```sh \n",
    "conda install beautifulsoup4=4.7.1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## **1. Importing Necessary Libraries**\n",
    "This first code block imports all the essential Python libraries for our task. We'll need `nltk` for text processing, `numpy` for numerical operations, `sklearn` for machine learning models and metrics, and `BeautifulSoup` to parse the XML data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Natural Language Toolkit for text processing tasks like tokenization.\n",
    "import nltk\n",
    "# Import NumPy for efficient numerical operations, especially on arrays.\n",
    "import numpy as np\n",
    "# Import scikit-learn's linear_model module, which contains Ridge and Lasso regression.\n",
    "from sklearn import linear_model\n",
    "# Import scikit-learn's metrics module to evaluate model performance (e.g., Mean Absolute Error).\n",
    "import sklearn.metrics\n",
    "# Import the BeautifulSoup library to easily parse XML and HTML files.\n",
    "from bs4 import BeautifulSoup\n",
    "# Import the CountVectorizer to convert text documents into a matrix of token counts.\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## **2. Defining the Data Reading Function**\n",
    "The following function, `read_movie_data`, is designed to read and parse the movie data from the specified XML file. It extracts movie reviews, their corresponding box office revenue (the target variable), and splits them into training and testing sets as defined within the file itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read and process the movie data from a given XML file.\n",
    "def read_movie_data(filename):\n",
    "    \n",
    "    # Initialize an empty list to store the review texts for the training set.\n",
    "    trainX=[]\n",
    "    # Initialize an empty list to store the box office revenues for the training set.\n",
    "    Y_train=[]\n",
    "    \n",
    "    # Initialize an empty list to store the review texts for the test set.\n",
    "    testX=[]\n",
    "    # Initialize an empty list to store the box office revenues for the test set.\n",
    "    Y_test=[]\n",
    "    \n",
    "    # Open and read the specified file.\n",
    "    with open(filename) as file:\n",
    "        # Create a BeautifulSoup object to parse the XML content of the file.\n",
    "        soup=BeautifulSoup(file)\n",
    "        # Find all XML tags named 'instance', where each tag represents a single movie.\n",
    "        movies=soup.findAll('instance')\n",
    "        # Iterate through each movie found in the file.\n",
    "        for movie in movies:\n",
    "            # Get the value of the 'subpop' attribute to determine if it's for training or testing.\n",
    "            split=movie[\"subpop\"]\n",
    "            # Find the 'regy' tag and get the box office value from its 'yvalue' attribute, converting it to a float.\n",
    "            y=float(movie.find('regy')[\"yvalue\"])\n",
    "\n",
    "            # we'll just take the first review in the data (each movie has multiple reviews)\n",
    "            review=movie.find('text')\n",
    "            \n",
    "            # Use NLTK's word_tokenize to split the review text into a list of words (tokens).\n",
    "            tokens=nltk.word_tokenize(review.text)\n",
    "            # Join the tokens back into a single string, separated by spaces.\n",
    "            words=' '.join(tokens)\n",
    "            # Check if this movie belongs to the training set.\n",
    "            if split == \"train\":\n",
    "                # If so, add the review text to the training feature list.\n",
    "                trainX.append(words)\n",
    "                # And add the box office value to the training target list.\n",
    "                Y_train.append(y)\n",
    "            # Check if this movie belongs to the test set.\n",
    "            elif split == \"test\":\n",
    "                # If so, add the review text to the test feature list.\n",
    "                testX.append(words)\n",
    "                # And add the box office value to the test target list.\n",
    "                Y_test.append(y)\n",
    "   \n",
    "    # Return the four populated lists.\n",
    "    return trainX, Y_train, testX, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## **3. Defining the Weight Analysis Function**\n",
    "This helper function, `analyze_weights`, is created to inspect the coefficients (or \"weights\") that our trained model learns for each feature (word or n-gram). It identifies and prints the features that most strongly predict high box office revenue (large positive weights) and those that most strongly predict low revenue (large negative weights)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to analyze and display the most influential feature weights from a trained model.\n",
    "def analyze_weights(learned_model, vocab, num_to_print, printZero=True):\n",
    "    # Create a reverse vocabulary to map feature indices back to their corresponding words/n-grams.\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "\n",
    "    # Get the indices that would sort the model's coefficients in ascending order.\n",
    "    sort_index = np.argsort(learned_model.coef_)\n",
    "    \n",
    "    # Print the features with the highest positive coefficients.\n",
    "    # We iterate backwards through the last 'num_to_print' elements of the sorted indices.\n",
    "    for k in reversed(sort_index[-num_to_print:]):\n",
    "        # Check if the coefficient is non-zero or if we are instructed to print zero-value coefficients.\n",
    "        if learned_model.coef_[k] != 0 or printZero:\n",
    "            # Print the coefficient value and the corresponding feature name.\n",
    "            print (\"%.5f\\t%s\" % (learned_model.coef_[k], reverse_vocab[k] ))\n",
    "        \n",
    "    # Print a blank line for better readability.\n",
    "    print()\n",
    "\n",
    "    # Print the features with the most negative coefficients.\n",
    "    # We iterate through the first 'num_to_print' elements of the sorted indices.\n",
    "    for k in sort_index[:num_to_print]:\n",
    "        # Check if the coefficient is non-zero or if we are instructed to print zero-value coefficients.\n",
    "        if learned_model.coef_[k] != 0 or printZero:\n",
    "            # Print the coefficient value and the corresponding feature name.\n",
    "            print (\"%.5f\\t%s\" % (learned_model.coef_[k], reverse_vocab[k] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## **4. Loading and Splitting the Data**\n",
    "Here, we call the `read_movie_data` function to load the dataset. The function returns the texts and box office values, neatly separated into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the data reading function with the path to the dataset file.\n",
    "# Unpack the returned values into separate variables for training and testing data.\n",
    "trainX, Y_train, testX, Y_test=read_movie_data(\"../data/7domains-train-dev.tl.xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## **5. Vectorizing the Text Data**\n",
    "Machine learning models can't work with raw text. We need to convert the movie reviews into a numerical format. We use `CountVectorizer` to transform our text data into a \"bag-of-words\" representation. Each review becomes a vector where each element represents the presence or absence of a specific word or n-gram from our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the CountVectorizer with specific parameters.\n",
    "# max_features=10000: Limit the vocabulary to the 10,000 most frequent words/n-grams.\n",
    "# ngram_range=(1,2): Include both single words (unigrams) and pairs of adjacent words (bigrams) as features.\n",
    "# lowercase=True: Convert all text to lowercase before tokenizing.\n",
    "# strip_accents=None: Do not remove accents from characters.\n",
    "# binary=True: Use 1 for presence and 0 for absence of a feature, rather than its frequency count.\n",
    "vectorizer = CountVectorizer(max_features=10000, ngram_range=(1,2), lowercase=True, strip_accents=None, binary=True)\n",
    "\n",
    "# Learn the vocabulary from the training data and transform the training text into a numerical matrix.\n",
    "X_train = vectorizer.fit_transform(trainX)\n",
    "# Transform the test data using the same vocabulary learned from the training data.\n",
    "X_test = vectorizer.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## **6. Ridge Regression (L2 Regularization)**\n",
    "Ridge regression is linear regression with L2 regularization. How does varying the regularization strength affect the accuracy (Mean Absolute Error)? How does it affect the rank order of the most informative coefficients? Play around with the parameters of the `CountVectorizer` above (varying the number of `max_features`, increasing the `ngram_range` to include bigrams, trigrams, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher values of alpha mean stronger regularization.\n",
    "# Initialize a Ridge Regression model with an alpha (regularization strength) of 100.\n",
    "# fit_intercept=True tells the model to calculate an intercept term.\n",
    "ridge_regression = linear_model.Ridge(alpha=100, fit_intercept=True)\n",
    "# Train (fit) the Ridge model using the training feature matrix (X_train) and training target values (Y_train).\n",
    "ridge_regression.fit(X_train, (Y_train))\n",
    "# Use the trained model to make predictions on the test set.\n",
    "preds=ridge_regression.predict(X_test)\n",
    "# Calculate the Mean Absolute Error (MAE) between the model's predictions and the actual test values.\n",
    "mae=sklearn.metrics.mean_absolute_error(preds, (Y_test))\n",
    "# Print the calculated MAE, formatted to three decimal places.\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "# Call the analyze_weights function to see the 5 most positive and 5 most negative features.\n",
    "analyze_weights(ridge_regression, vectorizer.vocabulary_, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "## **7. Lasso Regression (L1 Regularization)**\n",
    "Lasso is linear regression with L1 regularization, which pressures coefficients to not only be close to zero, but **exactly zero**. Lasso provides features selection as a result of this, since parameters with a 0 value are effectively removed from the model. How does varying the regularization strength here affect the number of non-zero coefficients? How does it affect the rank order of the most informative coefficients?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a Lasso Regression model with an alpha of 100.\n",
    "# max_iter=10000 is set to ensure the optimization algorithm has enough iterations to converge.\n",
    "lasso = linear_model.Lasso(alpha=100, fit_intercept=True, max_iter=10000)\n",
    "# Train the Lasso model on the training data.\n",
    "lasso.fit(X_train, (Y_train))\n",
    "# Make predictions on the test data.\n",
    "preds=lasso.predict(X_test)\n",
    "# Calculate the Mean Absolute Error.\n",
    "mae=sklearn.metrics.mean_absolute_error(preds, (Y_test))\n",
    "# Print the MAE.\n",
    "print(\"MAE: %.3f\" % mae)\n",
    "\n",
    "# Initialize a counter for non-zero features.\n",
    "count=0\n",
    "# Loop through all the coefficients learned by the Lasso model.\n",
    "for val in lasso.coef_:\n",
    "    # If a coefficient is not zero, increment the counter.\n",
    "    count+=1 if val != 0 else 0\n",
    "\n",
    "# Print the total number of features that were not eliminated by the L1 regularization.\n",
    "print(\"Nonzero features: %s\\n\" % count)\n",
    "# Analyze the weights, but this time set printZero=False since we are only interested in the features Lasso kept.\n",
    "analyze_weights(lasso, vectorizer.vocabulary_, 5, printZero=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
