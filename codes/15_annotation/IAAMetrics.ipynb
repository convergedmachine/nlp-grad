{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook calculates interannotator agreement rates for data with categorical labels (using Cohen's $\\kappa$) and real values (using Krippendorff's $\\alpha$).  We calculate these rates for two tasks: subjectivity/objectivity and suspense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 2: Importing Libraries\n",
    "This first code block imports the necessary classes and functions from the `nltk` (Natural Language Toolkit) library. It also imports the `sys` module, although it is not used in the provided code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import AnnotationTask, which is the main class for calculating agreement scores.\n",
    "from nltk.metrics.agreement import AnnotationTask\n",
    "\n",
    "# Import distance metrics. interval_distance is for real-valued data (like 1.0, 2.5, 5.0).\n",
    "# binary_distance is for categorical data where labels are either the same or different (e.g., \"A\" vs \"B\").\n",
    "from nltk.metrics import interval_distance, binary_distance \n",
    "\n",
    "# Import the sys module for system-specific parameters and functions (not used in this notebook).\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 3: Krippendorff's Alpha Function\n",
    "This cell defines a function to calculate Krippendorff's $\\alpha$ (alpha). This metric is ideal for measuring agreement between annotators when the data is numerical (integer, or real-valued) or ordinal. It can handle multiple raters and missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a list of annotation triples as input.\n",
    "# A triple is in the format (annotator_id, item_id, label).\n",
    "def krippendorff_alpha(annotation_triples):\n",
    "\n",
    "    # Create an AnnotationTask instance. \n",
    "    # The 'distance' parameter is set to interval_distance because we're dealing with real-valued data.\n",
    "    t = AnnotationTask(annotation_triples, distance=interval_distance)\n",
    "    \n",
    "    # Calculate Krippendorff's alpha for the task.\n",
    "    result = t.alpha()\n",
    "    \n",
    "    # Print the result, formatted to three decimal places.\n",
    "    print(\"%.3f\" % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 4: Cohen's Kappa Function\n",
    "This cell defines a function to calculate Cohen's $\\kappa$ (kappa). This metric is typically used to measure agreement between **two** annotators on a **categorical** task.\n",
    "\n",
    "*Note: The code uses `interval_distance` here. For a classic Cohen's Kappa with binary or nominal categories, `binary_distance` would be more appropriate. Using `interval_distance` makes it a weighted kappa.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a list of annotation triples as input.\n",
    "def cohens_kappa(annotation_triples):\n",
    "\n",
    "    # Create an AnnotationTask instance.\n",
    "    # Note: interval_distance is used, which makes this a weighted kappa.\n",
    "    t = AnnotationTask(annotation_triples, distance=interval_distance)\n",
    "    \n",
    "    # Calculate Cohen's kappa for the task.\n",
    "    result = t.kappa()\n",
    "    \n",
    "    # Print the result, formatted to three decimal places.\n",
    "    print(\"%.3f\" % result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 5: File Reading Function\n",
    "This function, `read_annos`, is designed to read a tab-separated values (`.tsv`) file. It assumes the file has two columns: the annotation value and the corresponding sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to read annotations from a given filename.\n",
    "def read_annos(filename):\n",
    "    # Initialize an empty list to store annotation scores.\n",
    "    annos=[]\n",
    "    # Initialize an empty list to store the sentences.\n",
    "    sentences=[]\n",
    "    # Open the specified file with UTF-8 encoding.\n",
    "    with open(filename, encoding=\"utf-8\") as file:\n",
    "\n",
    "        # Read the first line (header) and split it by tabs, but don't use it.\n",
    "        header=file.readline().rstrip().split(\"\\t\")\n",
    "            \n",
    "        # Loop through the rest of the lines in the file.\n",
    "        for line in file:\n",
    "            # Strip whitespace from the end of the line and split it into columns by tabs.\n",
    "            cols=line.rstrip().split(\"\\t\")\n",
    "            # Convert the first column to a float and add it to the 'annos' list.\n",
    "            annos.append(float(cols[0]))\n",
    "            # Add the second column (the sentence) to the 'sentences' list.\n",
    "            sentences.append(cols[1])\n",
    "    # Return the two lists: one for annotations and one for sentences.\n",
    "    return annos, sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 6: Data Conversion Function\n",
    "This function, `convert_anno_list`, transforms a simple list of annotation scores into the structured format required by the NLTK `AnnotationTask` class. The required format is a list of tuples, where each tuple is `(annotator_id, item_id, label)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that takes a list of annotations and an annotator ID.\n",
    "def convert_anno_list(annos, annotator_id):\n",
    "    # Initialize an empty list to store the converted data.\n",
    "    converted=[]\n",
    "    # Loop through the annotations list with both index (idx) and value (anno).\n",
    "    for idx, anno in enumerate(annos):\n",
    "        # Create a tuple with the annotator's ID, the item's index, and the annotation value.\n",
    "        # Then, append this tuple to the 'converted' list.\n",
    "        converted.append((annotator_id, idx, anno))\n",
    "    # Return the newly formatted list of annotation triples.\n",
    "    return converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 7 & 8: File Path Placeholders\n",
    "These cells are placeholders where you must specify the paths to your two annotation files. One file is for the first annotator, and the other is for the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the file path for the first annotator's data to a variable.\n",
    "# YOU MUST REPLACE \"path to your filename name\" WITH YOUR ACTUAL FILE PATH.\n",
    "anno1_filename=\"path to your filename name\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the file path for the second annotator's data to a variable.\n",
    "# YOU MUST REPLACE \"path to group annotation file here\" WITH YOUR ACTUAL FILE PATH.\n",
    "anno2_filename=\"path to group annotation file here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 9 & 10: Loading Annotation Data\n",
    "These cells use the `read_annos` function defined earlier to load the data from the specified file paths into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the read_annos function to load data for the first annotator.\n",
    "# 'anno1' will contain the scores, and 'sentences' will contain the corresponding text.\n",
    "anno1, sentences=read_annos(anno1_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the read_annos function to load data for the second annotator.\n",
    "# We only need the scores ('anno2'), so we use '_' to ignore the sentences list.\n",
    "anno2, _=read_annos(anno2_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 11: Data Validation\n",
    "This is a crucial sanity check. It ensures that both annotators have rated the exact same number of items. If the counts differ, inter-annotator agreement cannot be calculated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the number of annotations in the first list is different from the second.\n",
    "if len(anno1) != len(anno2):\n",
    "    # If they are different, print an error message showing the two different counts.\n",
    "    print (\"Different number of annotations: %s vs. %s\" % len(anno1), len(anno2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 12: Error Analysis\n",
    "This loop is for manual error analysis. It iterates through the annotations and prints out the items where the two annotators disagreed significantly (in this case, by a score of 1.0 or more). This helps identify ambiguous items or potential misunderstandings of the annotation guidelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out sentences with different annotations to see where annotators disagreed.\n",
    "# Loop through the indices from 0 to the total number of annotations.\n",
    "for idx in range(len(anno1)):\n",
    "    # Check if the absolute difference between the two annotators' scores is 1 or greater.\n",
    "    if abs(anno1[idx]-anno2[idx]) >= 1:\n",
    "        # If the disagreement is large, print the scores from both annotators and the sentence itself.\n",
    "        print(\"%s\\t%s\\t%s\" % (anno1[idx], anno2[idx], sentences[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 13 & 14: Formatting Data for NLTK\n",
    "Here, the loaded annotation lists are converted into the `(annotator, item, label)` triple format that the `AnnotationTask` class requires. Annotator 1 is assigned ID `0`, and Annotator 2 is assigned ID `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the first annotator's list into the required format, using annotator ID 0.\n",
    "anno1=convert_anno_list(anno1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the second annotator's list into the required format, using annotator ID 1.\n",
    "anno2=convert_anno_list(anno2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectivity is a binary rating, so use Cohen's $\\kappa$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 16: Executing Cohen's Kappa\n",
    "This cell combines the two formatted annotation lists and passes them to the `cohens_kappa` function to calculate and print the agreement score for the binary task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two formatted annotation lists into a single list.\n",
    "# Then, pass this combined list to the cohens_kappa function to calculate the score.\n",
    "cohens_kappa(anno1 + anno2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suspense is a real-valued rating, so use Krippendorff's $\\alpha$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cell 18: Executing Krippendorff's Alpha\n",
    "Finally, this cell calculates the agreement for the real-valued task (suspense) by passing the combined data to the `krippendorff_alpha` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the two formatted annotation lists into a single list.\n",
    "# Then, pass this combined list to the krippendorff_alpha function to calculate the score.\n",
    "krippendorff_alpha(anno1 + anno2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
