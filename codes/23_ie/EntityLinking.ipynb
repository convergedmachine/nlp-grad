{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to solve the problem of **Named Entity Disambiguation**, which is linking a mention of an entity (like \"Michael Jordan\") in a text to the correct entry in a knowledge base (like Wikipedia). It explores two common techniques: String Similarity and Cosine Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the 'wikipedia' library to fetch content from Wikipedia pages.\n",
    "import wikipedia\n",
    "# Import 'spacy' for natural language processing tasks, like identifying entities.\n",
    "import spacy\n",
    "# Import 'numpy' for efficient numerical operations, especially with vectors.\n",
    "import numpy as np\n",
    "# Import 'Counter' from the collections module for counting items (though not used in the final code, it's common in NLP).\n",
    "from collections import Counter\n",
    "# Import the 'math' module for basic mathematical functions.\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the NLP Model\n",
    "\n",
    "Here, we load a pre-trained English language model from the `spaCy` library. We disable the 'parser' component because we only need the model for tokenization and named entity recognition (NER), which makes it load faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 'en' model from spaCy. This model is trained on English text.\n",
    "# We disable the 'parser' to make the loading and processing faster since we don't need dependency parsing.\n",
    "nlp = spacy.load('en', disable=['parser'])\n",
    "# This is a common workaround if the standard 'en' model link is not installed.\n",
    "# It explicitly loads a small English web corpus.\n",
    "# nlp = spacy.load('en_core_web_sm', disable=['parser'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch Candidate Wikipedia Pages\n",
    "\n",
    "Our goal is to figure out which \"Michael Jordan\" a text is referring to. We'll use Wikipedia as our knowledge base. First, we select the three most likely candidates and download their summary content using the `wikipedia` library. This content will serve as the context for each entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary to map the official Wikipedia page titles to a more descriptive 'type' for easy reference.\n",
    "jordan_type = {\n",
    "    'Michael Jordan': 'NBA',\n",
    "    'Michael B. Jordan': 'actor',\n",
    "    'Michael I. Jordan': 'professor'\n",
    "}\n",
    "\n",
    "# A list of the exact Wikipedia page titles for our candidate entities.\n",
    "page_titles = ['Michael Jordan', 'Michael B. Jordan', 'Michael I. Jordan']\n",
    "\n",
    "# Create an empty list to store the data we download from Wikipedia.\n",
    "pages = []\n",
    "# Loop through each title in our list of page titles.\n",
    "for title in page_titles:\n",
    "    # Print a message to show the progress.\n",
    "    print(\"downloading '{}'\".format(title))\n",
    "    # Fetch the Wikipedia page object using the title.\n",
    "    page = wikipedia.page(title)\n",
    "    # Append a tuple containing the page's title and its summary text to our 'pages' list.\n",
    "    pages.append((page.title, page.summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Text Data for Disambiguation\n",
    "\n",
    "Now, we need some example sentences that mention \"Michael Jordan.\" These sentences are stored in a CSV file. The function below reads this file, and for each sentence (line), it uses `spaCy` to process it into a `doc` object. This `doc` object contains tokens and recognized entities, which we will use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename):\n",
    "    \n",
    "    \"\"\" Read data from file \n",
    "    Input: \n",
    "        - filename containing one document per line\n",
    "    Output:\n",
    "        - a list of spaCy tokenized documents\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the processed documents.\n",
    "    data=[]\n",
    "    # Open the specified file for reading.\n",
    "    with open(filename) as file:\n",
    "        # Iterate over each line in the file.\n",
    "        for line in file:\n",
    "            # Process the line with our nlp model to create a spaCy doc object.\n",
    "            # .strip() removes any leading/trailing whitespace.\n",
    "            doc = nlp(line.strip())\n",
    "            # Add the processed doc to our data list.\n",
    "            data.append(doc)\n",
    "    \n",
    "    # Print how many documents were loaded.\n",
    "    print(\"loaded {} docs\".format(len(data)))\n",
    "    \n",
    "    # Return the list of spaCy documents.\n",
    "    return data\n",
    "\n",
    "# Call the function to read our test data from the specified CSV file.\n",
    "docs = read_data(\"../data/MichaelJordan.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. String Similarity\n",
    "\n",
    "Our first approach is a simple one: compare the text of the entity found in our sentence (e.g., \"Michael Jordan\") directly with the titles of our candidate Wikipedia pages (e.g., \"Michael Jordan,\" \"Michael B. Jordan\"). We'll use the **Jaccard Similarity** metric, which measures the overlap between two sets. For strings, it's the ratio of the number of common words to the total number of unique words in both strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_similarity(str1, str2):\n",
    "    \"\"\"\n",
    "    Return a string similarity score betweein 0 and 1, \n",
    "    where 1 indicates exact match, 0 indicates no match\n",
    "    \n",
    "    e.g. string_similarity('hello world', 'hello world') = 1\n",
    "    \"\"\"\n",
    "    \n",
    "    # This implements word-level Jaccard Similarity.\n",
    "    # Convert the first string into a set of unique words.\n",
    "    str1 = set(str1.split())\n",
    "    # Convert the second string into a set of unique words.\n",
    "    str2 = set(str2.split())\n",
    "    # Calculate the intersection (common words) and union (total unique words).\n",
    "    # The similarity is the size of the intersection divided by the size of the union.\n",
    "    return float(len(str1 & str2)) / len(str1 | str2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the String Similarity Function\n",
    "\n",
    "Let's test our `string_similarity` function with a few examples to make sure it's working as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_string_similarity(str1, str2):\n",
    "    # A helper function to print the comparison and the resulting similarity score.\n",
    "    print(\"'{}' vs. '{}' = {}\".format(str1, str2, string_similarity(str1, str2)))\n",
    "    \n",
    "# Test case 1: Identical strings should return a score of 1.0.\n",
    "check_string_similarity(\"hello world\", \"hello world\")\n",
    "# Test case 2: Similar strings. They share \"Michael\" and \"Jordan\".\n",
    "check_string_similarity(\"Michael B. Jordan\", \"Michael I. Jordan\")\n",
    "# Test case 3: Partially similar strings. They share \"Clinton\".\n",
    "check_string_similarity(\"Hillary Clinton\", \"Bill Clinton\")\n",
    "# Test case 4: Completely different strings should return a score of 0.0.\n",
    "check_string_similarity(\"one two three\", \"four five six\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Candidates by String Similarity\n",
    "\n",
    "This function iterates through each of our test sentences (`docs`). For each sentence, it finds all named entities (like \"Michael Jordan\"). It then calculates the string similarity between each entity and each candidate Wikipedia page title. Finally, it selects the Wikipedia page with the highest similarity score as the best match for that sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_string_similarity(docs, pages):\n",
    "    \"\"\"\n",
    "    For each doc return the Wiki page that is the best match based on string similarity.\n",
    "    Returns a list of (title, score) tuples representing the best match for each doc.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the results for each document.\n",
    "    result = []\n",
    "    # Loop through each document in our test data.\n",
    "    for doc in docs:\n",
    "        # Initialize the best score to -1 (so any valid score will be higher).\n",
    "        best_score = -1\n",
    "        # Initialize the best page to the first page by default.\n",
    "        best_page = pages[0]\n",
    "        \n",
    "        # doc.ents contains all named entities recognized by spaCy in the document.\n",
    "        for e in doc.ents:\n",
    "            # For each entity, compare it against all candidate Wikipedia pages.\n",
    "            for page in pages:\n",
    "                # Calculate the string similarity between the entity's text and the page's title.\n",
    "                sim = string_similarity(e.text, page[0])\n",
    "                # If this similarity score is the highest we've seen so far for this doc...\n",
    "                if sim > best_score:\n",
    "                    # ...update the best score.\n",
    "                    best_score = sim\n",
    "                    # ...and update the best page.\n",
    "                    best_page = page\n",
    "        \n",
    "        # After checking all entities and pages, append the best match (title and score) to our results.\n",
    "        result.append((best_page[0], best_score))\n",
    "    # Return the list of best matches.\n",
    "    return result\n",
    "    \n",
    "def display_result(closest, docs):\n",
    "    \n",
    "    \"\"\" Print results in a readable format \"\"\"\n",
    "\n",
    "    # Iterate through the documents with an index.\n",
    "    for i in range(len(docs)):\n",
    "        # Print the original document text.\n",
    "        print(docs[i])\n",
    "        # Get the title of the best-matching Wikipedia page from the results.\n",
    "        title = closest[i][0]\n",
    "        # Print the best match title, its type (e.g., 'NBA'), and the similarity score.\n",
    "        print(\"{} - {} ({})\".format(title, jordan_type[title], closest[i][1]))\n",
    "        # Print a blank line for better readability.\n",
    "        print()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate String Similarity Performance\n",
    "\n",
    "Now let's run the ranking and see the results. As you'll see, this method works well when the entity mention is an exact match for a page title, but it fails when the context is needed for disambiguation (like in the \"computer science\" example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ranking function to get the best matches based on string similarity.\n",
    "string_rank = rank_string_similarity(docs, pages)\n",
    "# Display the results in a formatted way.\n",
    "display_result(string_rank, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Cosine Similarity\n",
    "\n",
    "String similarity alone isn't enough because it ignores context. A better approach is to compare the **meaning** of the text surrounding the entity with the meaning of the Wikipedia page summary. We can do this using **word embeddings** (like GloVe) to convert text into numerical vectors. Then, we can calculate the **cosine similarity** between these vectors. A high cosine similarity (close to 1) means the vectors point in a similar direction, implying similar semantic content.\n",
    "\n",
    "First, we load pre-trained GloVe word embeddings. These embeddings map words to high-dimensional vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(filename, max_vocab_size):\n",
    "    \"\"\"\n",
    "    Load embeddings from a file.\n",
    "    Returns a numpy array of embeddings and a vocabulary dictionary {word: index}.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize a dictionary to map words to their index in the embeddings list.\n",
    "    vocab={}\n",
    "    # Initialize a list to hold the embedding vectors.\n",
    "    embeddings=[]\n",
    "    # Open the embeddings file.\n",
    "    with open(filename) as file:\n",
    "        \n",
    "        # The first line of the w2v format file contains the number of words and the vector size.\n",
    "        cols=file.readline().split(\" \")\n",
    "        num_words=int(cols[0])\n",
    "        size=int(cols[1])\n",
    "        # Add a zero vector for padding (index 0).\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Add a zero vector for unknown words (UNK) (index 1).\n",
    "        embeddings.append(np.zeros(size))\n",
    "        # Add special tokens to our vocabulary.\n",
    "        vocab[\"_0_\"]=0\n",
    "        vocab[\"_UNK_\"]=1\n",
    "        \n",
    "        # Iterate over each line in the file, where each line is a word and its vector.\n",
    "        for idx,line in enumerate(file):\n",
    "\n",
    "            # Stop if we reach the maximum desired vocabulary size.\n",
    "            if idx+2 >= max_vocab_size:\n",
    "                break\n",
    "\n",
    "            # Split the line into the word and the vector values.\n",
    "            cols=line.rstrip().split(\" \")\n",
    "            # Convert the vector values to a numpy array of floats.\n",
    "            val=np.array(cols[1:])\n",
    "            # The first column is the word.\n",
    "            word=cols[0]\n",
    "            \n",
    "            # Add the vector to our embeddings list.\n",
    "            embeddings.append(val)\n",
    "            # Add the word and its index to our vocabulary. The index is idx+2 due to padding/UNK tokens.\n",
    "            vocab[word]=idx+2\n",
    "\n",
    "    # Convert the list of embeddings to a single numpy array for efficiency.\n",
    "    return np.array(embeddings), vocab\n",
    "\n",
    "# Load the top 50,000 word embeddings from the GloVe file.\n",
    "embeddings, vocab=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-Time Conversion of GloVe file\n",
    "\n",
    "This cell is a utility script that only needs to be run once. The raw GloVe file format is slightly different from the `word2vec` format that our `load_embeddings` function expects. This code uses the `gensim` library to perform the conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run after downloading glove.42B.300d.50K.txt for the first time.\n",
    "Only run this cell if you got a 'file not found' error above.\n",
    "\"\"\"\n",
    "# Import libraries from gensim for handling word vectors.\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Define the path to the raw GloVe file.\n",
    "glove_file=\"../data/glove.42B.300d.50K.txt\"\n",
    "# Define the path for the output file in word2vec format.\n",
    "glove_in_w2v_format=\"../data/glove.42B.300d.50K.w2v.txt\"\n",
    "# Run the conversion utility. The underscore '_' is used to ignore the function's return value.\n",
    "_ = glove2word2vec(glove_file, glove_in_w2v_format)\n",
    "\n",
    "# Now, load the embeddings from the newly created w2v-formatted file.\n",
    "embeddings, vocab=load_embeddings(\"../data/glove.42B.300d.50K.w2v.txt\", 50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Document Vector Representations\n",
    "\n",
    "To compare two pieces of text (like a sentence and a Wikipedia summary), we need to represent each one as a single vector. A simple and effective way to do this is to get the embedding for every word in the text and then calculate the average of all those vectors. This function does exactly that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_representation(doc, vocab, embeddings):\n",
    "    \"\"\"\n",
    "    Return one vector that represents the entire array of input tokens\n",
    "    by averaging the word embeddings of all words in the document.\n",
    "    \"\"\"\n",
    "    \n",
    "    # This implementation takes the average of all word embeddings in the doc.\n",
    "    # Create an empty list to store the embedding vectors of words found in the vocab.\n",
    "    words = []\n",
    "    # Iterate through each token (word) in the document.\n",
    "    for token in doc:\n",
    "        # Check if the lowercase version of the token exists in our vocabulary.\n",
    "        if token.lower() in vocab:\n",
    "            # If it exists, get its vector from the embeddings matrix and add it to our list.\n",
    "            words.append(embeddings[vocab[token.lower()]].astype(np.float))\n",
    "    # Calculate the mean of all vectors along axis 0 (column-wise average).\n",
    "    # This results in a single vector representing the entire document.\n",
    "    return np.mean(words, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rank Candidates by Cosine Similarity\n",
    "\n",
    "This is the main function for our second approach. It works as follows:\n",
    "1. It first converts each candidate Wikipedia page's summary into a single vector representation using our `get_doc_representation` function.\n",
    "2. Then, it iterates through each of our test sentences (`docs`).\n",
    "3. For each sentence, it also creates a single vector representation.\n",
    "4. It then calculates the cosine similarity between the sentence's vector and each of the Wikipedia page vectors.\n",
    "5. The page with the highest cosine similarity score is chosen as the best match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(v1, v2):\n",
    "    \"\"\" Returns cosine similarity between two vectors \"\"\"\n",
    "    # Calculate the dot product of the two vectors.\n",
    "    # Divide by the product of the magnitudes (norms) of the vectors.\n",
    "    cos = np.dot(v1, v2) / (np.sqrt(np.dot(v1,v1)) * np.sqrt(np.dot(v2,v2)))\n",
    "    return cos\n",
    "\n",
    "def rank_cos_similarity(docs, pages, vocab, embeddings):\n",
    "    \"\"\"\n",
    "    For each doc return the Wiki page that is the best match based on cosine similarity.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize an empty list to store the final results.\n",
    "    result = []\n",
    "    \n",
    "    # Pre-calculate the vector representations for each Wikipedia page summary.\n",
    "    page_representations = []\n",
    "    for p in pages:\n",
    "        # The summary text is the second element in the tuple.\n",
    "        summary = p[1]\n",
    "        # Get the vector representation for the summary. Note: summary is split into words first.\n",
    "        emb = get_doc_representation(summary.split(\" \"), vocab, embeddings)\n",
    "        # Store the page title along with its vector representation.\n",
    "        page_representations.append((p[0], emb))\n",
    "    \n",
    "    # Now, process each input document.\n",
    "    for doc in docs:     \n",
    "        # Get the vector representation for the entire spaCy doc.\n",
    "        # We extract the text of each token before passing it to the function.\n",
    "        doc_representation = get_doc_representation([token.text for token in doc], vocab, embeddings)\n",
    "        \n",
    "        # A list to store the similarity scores for the current doc against all pages.\n",
    "        scores = []\n",
    "        \n",
    "        # Iterate through the pre-calculated page representations.\n",
    "        for p in page_representations:\n",
    "            title = p[0] # The page title\n",
    "            emb = p[1]   # The page's vector\n",
    "            # Calculate the cosine similarity between the doc vector and the page vector.\n",
    "            sim = cos_similarity(doc_representation, emb)\n",
    "            # Append the title and its similarity score to the scores list.\n",
    "            scores.append((p[0], sim))\n",
    "\n",
    "        # Sort the scores in descending order based on the similarity score (the second element, x[1]).\n",
    "        scores = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "        # The best match is the first item in the sorted list. Append it to the results.\n",
    "        result.append(scores[0])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate Cosine Similarity Performance\n",
    "\n",
    "Let's execute the cosine similarity ranking. Observe the results. The context of \"computer science\" in the first sentence now correctly helps the model link \"Michael Jordan\" to the professor, demonstrating that this method is much more powerful than simple string matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the ranking function using cosine similarity.\n",
    "closest_cos = rank_cos_similarity(docs, pages, vocab, embeddings)\n",
    "# Display the results using the same helper function.\n",
    "display_result(closest_cos, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Other Features?\n",
    "\n",
    "Based on your results above, can you think of other features that might help improve entity linking performance further? Name one feature and justify why you think it is a good feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One powerful feature would be **Popularity** or **Priors**. This feature represents the overall probability that a mention of a name (like \"Michael Jordan\") refers to a specific entity, regardless of the context. For example, the NBA player Michael Jordan is globally far more famous than the professor or the actor. We could determine this by looking at Wikipedia page view counts or the number of incoming links to each page. By incorporating this as a feature, if the context is ambiguous, the model can default to the most probable entity. This would be combined with the context similarity score (like cosine similarity) to make a final, more robust decision. For instance, the final score could be a weighted average: `Score = 0.7 * CosineSimilarity + 0.3 * PopularityScore`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "loc",
   "language": "python",
   "name": "loc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
